{"cells":[{"cell_type":"code","source":["DF = sqlContext.read.format('com.databricks.spark.csv').options(delimiter=',', header='true', inferschema='true').load(\"dbfs:/mnt/s3/data/train_v2_flatten.csv\")\n#display(DF)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":1},{"cell_type":"code","source":["import pyspark.sql.functions as F\nimport pyspark.sql.types as T\ndef deleteNull_castToFloat(x):\n  \n  if x == None:\n    return 0.0\n  if x == '(not set)':\n    return 0\n  if x == 'No':\n    return 0.0\n  else:\n    y = len(x)\n    if y>6:\n      i = int(x[ : y-6])\n      f = int(x[y-6:y])\n    else:\n      i = 0\n      f = int(x)\n    a = float(i)\n    b = float(f)/1000000\n    return a+b\nudf_deleteNull_castToFloat = F.udf(deleteNull_castToFloat, T.FloatType())\n\ndef deleteNull_castToInt(x):\n  \n  if x == None:\n    return 0\n  if x == '(not set)':\n    return 0\n  if x == 'No':\n    return 0\n  else:\n    return int(x)\nudf_deleteNull_castToInt = F.udf(deleteNull_castToInt, T.IntegerType())\n\ndef deleteNull_castToInt_visitNumber(x):\n  \n  if x == None:\n    return 0\n  if x == '(not set)':\n    return 0\n  if x == 'No':\n    return 0\n  if len(x)>10:\n    return 0\n  else:\n    return int(x)\nudf_deleteNull_castToInt_visitNumber = F.udf(deleteNull_castToInt_visitNumber, T.IntegerType())"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":2},{"cell_type":"code","source":["from pyspark.sql.functions import col\nDF_select = DF.select(udf_deleteNull_castToFloat(\"totals_transactionRevenue\").alias(\"revenue\"), udf_deleteNull_castToFloat(\"totals_totalTransactionRevenue\").alias(\"total_revenue\"), \"device_operatingSystem\", \"device_browser\",  \"geoNetwork_country\", \"channelGrouping\", (col('visitStartTime')).cast(\"string\").alias(\"startTime_cast\"), (col('date')).cast(\"string\").alias(\"date_cast\"), \"fullVisitorId\", udf_deleteNull_castToInt(\"totals_hits\").alias(\"hits\"), udf_deleteNull_castToInt(\"totals_pageviews\").alias(\"pageview\"), udf_deleteNull_castToInt_visitNumber(\"visitNumber\").alias(\"visitNumber\"),\n                     \"geoNetwork_networkDomain\",\"geoNetwork_region\" )"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":3},{"cell_type":"code","source":["from pyspark.sql.functions import unix_timestamp, to_date\nfrom pyspark.sql.functions import from_unixtime\nfrom pyspark.sql.functions import month, dayofweek, hour, year, weekofyear, dayofyear\n\nDF_select = DF_select.withColumn('date_cast', to_date('date_cast', 'yyyyMMdd'))\n#DF_select = DF_select.withColumn(\"month\", month(\"date_cast\"))\n#DF_select = DF_select.withColumn(\"weekday\", dayofweek(\"date_cast\"))\n#DF_select = DF_select.withColumn(\"year\", year(\"date_cast\"))\nDF_select = DF_select.withColumn(\"yearweek\", weekofyear(\"date_cast\"))\nDF_select = DF_select.withColumn(\"yearday\", dayofyear(\"date_cast\"))\n#DF_select = DF_select.withColumn(\"startTime_timestamp\", from_unixtime(\"startTime_cast\"))\n#DF_select = DF_select.withColumn(\"hour\", hour(\"startTime_timestamp\"))\n#DF_select = DF_select.drop('startTime_timestamp', \"startTime_cast\", 'date_cast')\nDF_select.cache()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">33</span><span class=\"ansired\">]: </span>DataFrame[revenue: float, total_revenue: float, device_operatingSystem: string, device_browser: string, geoNetwork_country: string, channelGrouping: string, startTime_cast: string, date_cast: date, fullVisitorId: string, hits: int, pageview: int, visitNumber: int, geoNetwork_networkDomain: string, geoNetwork_region: string, yearweek: int, yearday: int]\n</div>"]}}],"execution_count":4},{"cell_type":"code","source":["DF_fea = DF_select"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":5},{"cell_type":"code","source":["#Generating features grouped by geoNetwork_networkDomain\n\n\n# pageview\nDF_fea = DF_fea.join(DF_fea.groupby('geoNetwork_networkDomain').sum('pageview').withColumnRenamed('sum(pageview)', 'sum_pageviews_per_network_domain'), \"geoNetwork_networkDomain\")\n\nDF_fea = DF_fea.join(DF_fea.groupby('geoNetwork_networkDomain').avg('pageview').withColumnRenamed('avg(pageview)', 'avg_pageviews_per_network_domain'), \"geoNetwork_networkDomain\")\n\nDF_fea = DF_fea.join(DF_fea.groupby('geoNetwork_networkDomain').agg(F.count('pageview')).withColumnRenamed('count(pageview)', 'count_pageviews_per_network_domain'),\"geoNetwork_networkDomain\")\n\n# hits\nDF_fea = DF_fea.join(DF_fea.groupby('geoNetwork_networkDomain').sum('hits').withColumnRenamed('sum(hits)', 'sum_hits_per_network_domain'), \"geoNetwork_networkDomain\")\n\nDF_fea = DF_fea.join(DF_fea.groupby('geoNetwork_networkDomain').avg('hits').withColumnRenamed('avg(hits)', 'avg_hits_per_network_domain'), \"geoNetwork_networkDomain\")\n\nDF_fea = DF_fea.join(DF_fea.groupby('geoNetwork_networkDomain').agg(F.count('hits')).withColumnRenamed('count(hits)', 'count_hits_per_network_domain'),\"geoNetwork_networkDomain\")\n\n# DF_fea = DF_fea.join(DF_fea.groupby('geoNetwork_networkDomain').sum('revenue').withColumnRenamed('sum(revenue)', 'sum_revenue_per_day'), \"geoNetwork_networkDomain\")\n\n# DF_fea = DF_fea.join(DF_fea.groupby('geoNetwork_networkDomain').avg('revenue').withColumnRenamed('avg(revenue)', 'avg_revenue_per_day'), \"geoNetwork_networkDomain\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":6},{"cell_type":"code","source":["#Generating features grouped by dayofyear & weekofyear\n\nDF_yearday = DF_fea.groupby('yearday').sum('hits').withColumnRenamed('sum(hits)', 'sum_hits_per_day').withColumnRenamed(\"yearday\", 'yearday2')\n\nDF_fea = DF_fea.join(DF_yearday, DF_yearday.yearday2 == DF_fea.yearday).drop('yearday2')\n\nDF_yearday = DF_fea.groupby('yearday').avg('hits').withColumnRenamed('avg(hits)', 'avg_hits_per_day').withColumnRenamed(\"yearday\", 'yearday2')\n\nDF_fea = DF_fea.join(DF_yearday, DF_yearday.yearday2 == DF_fea.yearday).drop('yearday2')\n\nDF_yearweek = DF_fea.groupby('yearweek').sum('hits').withColumnRenamed('sum(hits)', 'sum_hits_per_week').withColumnRenamed(\"yearweek\", 'yearweek2')\n\nDF_fea = DF_fea.join(DF_yearweek, DF_yearweek.yearweek2 == DF_fea.yearweek).drop('yearweek2')\n\nDF_yearweek = DF_fea.groupby('yearweek').avg('hits').withColumnRenamed('avg(hits)', 'avg_hits_per_week').withColumnRenamed(\"yearweek\", 'yearweek2')\n\nDF_fea = DF_fea.join(DF_yearweek, DF_yearweek.yearweek2 == DF_fea.yearweek).drop('yearweek2')\n\n# DF_fea = DF_fea.join(DF_fea.groupby('dayofyear').sum('revenue').withColumnRenamed('sum(revenue)', 'sum_revenue_per_day'), \"dayofyear\")\n\n# DF_fea = DF_fea.join(DF_fea.groupby('dayofyear').avg('revenue').withColumnRenamed('avg(revenue)', 'avg_revenue_per_day'), \"dayofyear\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":7},{"cell_type":"code","source":["#Generating features grouped by geoNetwork_region\n\n#DF_fea = DF_fea.join(DF_fea.groupby('geoNetwork_region').sum('pageview').withColumnRenamed('sum(pageview)', 'sum_pageviews_per_region'), \"geoNetwork_region\")\n\nDF_fea = DF_fea.join(DF_fea.groupby('geoNetwork_region').avg('pageview').withColumnRenamed('avg(pageview)', 'avg_pageviews_per_region'), \"geoNetwork_region\")\n\n#DF_fea = DF_fea.join(DF_fea.groupby('geoNetwork_region').agg(F.count('pageview')).withColumnRenamed('count(pageview)', 'count_pageviews_per_region'),\"geoNetwork_region\")\n\n#DF_fea = DF_fea.join(DF_fea.groupby('geoNetwork_region').sum('hits').withColumnRenamed('sum(hits)', 'sum_hits_per_region'), \"geoNetwork_region\")\n\nDF_fea = DF_fea.join(DF_fea.groupby('geoNetwork_region').avg('hits').withColumnRenamed('avg(hits)', 'avg_hits_per_region'), \"geoNetwork_region\")\n\n#DF_fea = DF_fea.join(DF_fea.groupby('geoNetwork_region').agg(F.count('hits')).withColumnRenamed('count(hits)', 'count_hits_per_region'),\"geoNetwork_region\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":8},{"cell_type":"code","source":["#Generating features grouped by geoNetwork_country\n\nDF_fea = DF_fea.join(DF_fea.groupby('geoNetwork_country').sum('pageview').withColumnRenamed('sum(pageview)', 'sum_pageviews_per_country'), \"geoNetwork_country\")\n\nDF_fea = DF_fea.join(DF_fea.groupby('geoNetwork_country').avg('pageview').withColumnRenamed('avg(pageview)', 'avg_pageviews_per_country'), \"geoNetwork_country\")\n\nDF_fea = DF_fea.join(DF_fea.groupby('geoNetwork_country').agg(F.count('pageview')).withColumnRenamed('count(pageview)', 'count_pageviews_per_country'),\"geoNetwork_country\")\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\ncom.databricks.backend.daemon.driver.PythonDriverLocal$PythonException: Repl PythonDriver[ReplId-266aa-efcd7-cd3a4-7](stateStr) was not ready to run after 5, currently in state DriverCancelling\n\tat com.databricks.backend.daemon.driver.PythonDriverLocal.waitForDriverReadyAndRun(PythonDriverLocal.scala:490)\n\tat com.databricks.backend.daemon.driver.PythonDriverLocal.repl(PythonDriverLocal.scala:205)\n\tat com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$3.apply(DriverLocal.scala:248)\n\tat com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$3.apply(DriverLocal.scala:228)\n\tat com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:188)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n\tat com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:183)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:40)\n\tat com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:221)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:40)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:228)\n\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:595)\n\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:595)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:590)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:474)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:548)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:380)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:327)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:215)\n\tat java.lang.Thread.run(Thread.java:748)"]}}],"execution_count":9},{"cell_type":"code","source":["DF_fea = DF_fea.join(DF_fea.groupby('geoNetwork_country').sum('hits').withColumnRenamed('sum(hits)', 'sum_hits_per_country'), \"geoNetwork_country\")\n\nDF_fea = DF_fea.join(DF_fea.groupby('geoNetwork_country').avg('hits').withColumnRenamed('avg(hits)', 'avg_hits_per_country'), \"geoNetwork_country\")\n\nDF_fea = DF_fea.join(DF_fea.groupby('geoNetwork_country').agg(F.count('hits')).withColumnRenamed('count(hits)', 'count_hits_per_country'),\"geoNetwork_country\")"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["#Generating features grouped by fullVisitorId\n\nDF_fea = DF_fea.join(DF_fea.groupby('fullVisitorId').sum('pageview').withColumnRenamed('sum(pageview)', 'sum_pageviews_per_fullVisitorId'), \"fullVisitorId\")\n\n#DF_fea = DF_fea.join(DF_fea.groupby('fullVisitorId').avg('pageview').withColumnRenamed('avg(pageview)', 'avg_pageviews_per_fullVisitorId'), \"fullVisitorId\")\n\n#DF_fea = DF_fea.join(DF_fea.groupby('fullVisitorId').agg(F.count('pageview')).withColumnRenamed('count(pageview)', 'count_pageviews_per_fullVisitorId'),\"fullVisitorId\")\n\nDF_fea = DF_fea.join(DF_fea.groupby('fullVisitorId').sum('hits').withColumnRenamed('sum(hits)', 'sum_hits_per_fullVisitorId'), \"fullVisitorId\")\n\n#DF_fea = DF_fea.join(DF_fea.groupby('fullVisitorId').avg('hits').withColumnRenamed('avg(hits)', 'avg_hits_per_fullVisitorId'), \"fullVisitorId\")\n\n#DF_fea = DF_fea.join(DF_fea.groupby('fullVisitorId').agg(F.count('hits')).withColumnRenamed('count(hits)', 'count_hits_per_fullVisitorId'),\"fullVisitorId\")\n\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\ncom.databricks.backend.daemon.driver.PythonDriverLocal$PythonException: Repl PythonDriver[ReplId-266aa-efcd7-cd3a4-7](stateStr) was not ready to run after 5, currently in state DriverCancelling\n\tat com.databricks.backend.daemon.driver.PythonDriverLocal.waitForDriverReadyAndRun(PythonDriverLocal.scala:490)\n\tat com.databricks.backend.daemon.driver.PythonDriverLocal.repl(PythonDriverLocal.scala:205)\n\tat com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$3.apply(DriverLocal.scala:248)\n\tat com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$3.apply(DriverLocal.scala:228)\n\tat com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:188)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n\tat com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:183)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:40)\n\tat com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:221)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:40)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:228)\n\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:595)\n\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:595)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:590)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:474)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:548)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:380)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:327)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:215)\n\tat java.lang.Thread.run(Thread.java:748)"]}}],"execution_count":11},{"cell_type":"code","source":["sqlContext.registerDataFrameAsTable(DF_fea, \"DF\")\n\nDF_fea = DF_fea.join(sqlContext.sql(\"select fullVisitorId, sum_pageviews_per_fullVisitorId/avg_pageviews_per_region as user_pageviews_to_region from DF\"), \"fullVisitorId\")\n\nDF_fea = DF_fea.join(sqlContext.sql(\"select fullVisitorId, sum_hits_per_fullVisitorId/avg_hits_per_region as user_hits_to_region from DF\"), \"fullVisitorId\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\ncom.databricks.backend.daemon.driver.PythonDriverLocal$PythonException: Repl PythonDriver[ReplId-266aa-efcd7-cd3a4-7](stateStr) was not ready to run after 5, currently in state DriverCancelling\n\tat com.databricks.backend.daemon.driver.PythonDriverLocal.waitForDriverReadyAndRun(PythonDriverLocal.scala:490)\n\tat com.databricks.backend.daemon.driver.PythonDriverLocal.repl(PythonDriverLocal.scala:205)\n\tat com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$3.apply(DriverLocal.scala:248)\n\tat com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$3.apply(DriverLocal.scala:228)\n\tat com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:188)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n\tat com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:183)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:40)\n\tat com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:221)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:40)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:228)\n\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:595)\n\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:595)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:590)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:474)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:548)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:380)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:327)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:215)\n\tat java.lang.Thread.run(Thread.java:748)"]}}],"execution_count":12},{"cell_type":"code","source":["sqlContext.sql(\"select fullVisitorId, visitNumber/pageview as Test from DF\")"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["%sql\n\ninsert into DF select visitNumber/pageview as Test from DF"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["DF_select_group = DF_select.groupby('fullVisitorId').avg('visitNumber')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":15},{"cell_type":"code","source":["display(DF_select.join(DF_select_group,DF_select.fullVisitorId == DF_select_group.fullVisitorId))"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["# process geoNetwork_country\nimport numpy as np\ncountry_5k_10k = DF_select.groupBy(\"geoNetwork_country\").count().filter(\"count>5000\").filter(\"count<10000\").select('geoNetwork_country')\ncountry_0_5k = DF_select.groupBy(\"geoNetwork_country\").count().filter(\"count<=5000\").select('geoNetwork_country')\ncountry_5k_10k = np.array(country_5k_10k.toPandas().geoNetwork_country)\ncountry_0_5k = np.array(country_0_5k.toPandas().geoNetwork_country)\n\ndef process_country(x):\n  if x == None:\n    return 'Other'\n  elif x == '(not set)':\n    return 'Other'\n  elif len(x)>40:\n    return 'Other'\n  elif x in country_0_5k:\n    return 'Other_0_5k'\n  elif x in country_5k_10k:\n    return 'Other_5k_10k'\n  else:\n    return x\n  \n  \nudf_process_country = F.udf(process_country, T.StringType())\nDF_fea = DF_fea.withColumn('process_country', udf_process_country('geoNetwork_country'))\ndisplay(DF_fea.groupBy(\"process_country\").agg(F.count(\"process_country\")).orderBy(\"count(process_country)\"))\n                      "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;command-2487822865328475&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">      3</span> country_5k_10k <span class=\"ansiyellow\">=</span> DF_select<span class=\"ansiyellow\">.</span>groupBy<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;geoNetwork_country&quot;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>count<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>filter<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;count&gt;5000&quot;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>filter<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;count&lt;10000&quot;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>select<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;geoNetwork_country&apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      4</span> country_0_5k <span class=\"ansiyellow\">=</span> DF_select<span class=\"ansiyellow\">.</span>groupBy<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;geoNetwork_country&quot;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>count<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>filter<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;count&lt;=5000&quot;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>select<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;geoNetwork_country&apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">----&gt; 5</span><span class=\"ansiyellow\"> </span>country_5k_10k <span class=\"ansiyellow\">=</span> np<span class=\"ansiyellow\">.</span>array<span class=\"ansiyellow\">(</span>country_5k_10k<span class=\"ansiyellow\">.</span>toPandas<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>geoNetwork_country<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      6</span> country_0_5k <span class=\"ansiyellow\">=</span> np<span class=\"ansiyellow\">.</span>array<span class=\"ansiyellow\">(</span>country_0_5k<span class=\"ansiyellow\">.</span>toPandas<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>geoNetwork_country<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      7</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/dataframe.py</span> in <span class=\"ansicyan\">toPandas</span><span class=\"ansiblue\">(self)</span>\n<span class=\"ansigreen\">   2029</span>                 <span class=\"ansigreen\">raise</span> RuntimeError<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;%s\\n%s&quot;</span> <span class=\"ansiyellow\">%</span> <span class=\"ansiyellow\">(</span>_exception_message<span class=\"ansiyellow\">(</span>e<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> msg<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   2030</span>         <span class=\"ansigreen\">else</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">-&gt; 2031</span><span class=\"ansiyellow\">             </span>pdf <span class=\"ansiyellow\">=</span> pd<span class=\"ansiyellow\">.</span>DataFrame<span class=\"ansiyellow\">.</span>from_records<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">.</span>collect<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> columns<span class=\"ansiyellow\">=</span>self<span class=\"ansiyellow\">.</span>columns<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   2032</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   2033</span>             dtype <span class=\"ansiyellow\">=</span> <span class=\"ansiyellow\">{</span><span class=\"ansiyellow\">}</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/dataframe.py</span> in <span class=\"ansicyan\">collect</span><span class=\"ansiblue\">(self)</span>\n<span class=\"ansigreen\">    479</span>         <span class=\"ansired\"># Default path used in OSS Spark / for non-DF-ACL clusters:</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    480</span>         <span class=\"ansigreen\">with</span> SCCallSiteSync<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">.</span>_sc<span class=\"ansiyellow\">)</span> <span class=\"ansigreen\">as</span> css<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 481</span><span class=\"ansiyellow\">             </span>sock_info <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>_jdf<span class=\"ansiyellow\">.</span>collectToPython<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    482</span>         <span class=\"ansigreen\">return</span> list<span class=\"ansiyellow\">(</span>_load_from_socket<span class=\"ansiyellow\">(</span>sock_info<span class=\"ansiyellow\">,</span> BatchedSerializer<span class=\"ansiyellow\">(</span>PickleSerializer<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    483</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansicyan\">__call__</span><span class=\"ansiblue\">(self, *args)</span>\n<span class=\"ansigreen\">   1255</span>         answer <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>gateway_client<span class=\"ansiyellow\">.</span>send_command<span class=\"ansiyellow\">(</span>command<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1256</span>         return_value = get_return_value(\n<span class=\"ansigreen\">-&gt; 1257</span><span class=\"ansiyellow\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansigreen\">   1258</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1259</span>         <span class=\"ansigreen\">for</span> temp_arg <span class=\"ansigreen\">in</span> temp_args<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansicyan\">deco</span><span class=\"ansiblue\">(*a, **kw)</span>\n<span class=\"ansigreen\">     61</span>     <span class=\"ansigreen\">def</span> deco<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">*</span>a<span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">**</span>kw<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     62</span>         <span class=\"ansigreen\">try</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">---&gt; 63</span><span class=\"ansiyellow\">             </span><span class=\"ansigreen\">return</span> f<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">*</span>a<span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">**</span>kw<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     64</span>         <span class=\"ansigreen\">except</span> py4j<span class=\"ansiyellow\">.</span>protocol<span class=\"ansiyellow\">.</span>Py4JJavaError <span class=\"ansigreen\">as</span> e<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     65</span>             s <span class=\"ansiyellow\">=</span> e<span class=\"ansiyellow\">.</span>java_exception<span class=\"ansiyellow\">.</span>toString<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py</span> in <span class=\"ansicyan\">get_return_value</span><span class=\"ansiblue\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansigreen\">    326</span>                 raise Py4JJavaError(\n<span class=\"ansigreen\">    327</span>                     <span class=\"ansiblue\">&quot;An error occurred while calling {0}{1}{2}.\\n&quot;</span><span class=\"ansiyellow\">.</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 328</span><span class=\"ansiyellow\">                     format(target_id, &quot;.&quot;, name), value)\n</span><span class=\"ansigreen\">    329</span>             <span class=\"ansigreen\">else</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    330</span>                 raise Py4JError(\n\n<span class=\"ansired\">Py4JJavaError</span>: An error occurred while calling o1227.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 46.0 failed 1 times, most recent failure: Lost task 5.0 in stage 46.0 (TID 3943, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &quot;/databricks/spark/python/pyspark/worker.py&quot;, line 262, in main\n    process()\n  File &quot;/databricks/spark/python/pyspark/worker.py&quot;, line 257, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File &quot;/databricks/spark/python/pyspark/worker.py&quot;, line 183, in &lt;lambda&gt;\n    func = lambda _, it: map(mapper, it)\n  File &quot;&lt;string&gt;&quot;, line 1, in &lt;lambda&gt;\n  File &quot;/databricks/spark/python/pyspark/worker.py&quot;, line 79, in &lt;lambda&gt;\n    return lambda *a: f(*a)\n  File &quot;/databricks/spark/python/pyspark/util.py&quot;, line 55, in wrapper\n    return f(*args, **kwargs)\n  File &quot;&lt;command-2487822865328457&gt;&quot;, line 12, in deleteNull_castToFloat\nTypeError: object of type &apos;int&apos; has no len()\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:317)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:83)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:66)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:271)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:620)\n\tat org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$2$$anon$1.hasNext(InMemoryRelation.scala:157)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1092)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1083)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1018)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1083)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:809)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:347)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:298)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:42)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:336)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:300)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:42)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:336)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:300)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:42)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:336)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:300)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:42)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:336)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:300)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:112)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:384)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1747)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1735)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1734)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1734)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:962)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:962)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:962)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1970)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1918)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1906)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:759)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2141)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2238)\n\tat org.apache.spark.sql.execution.collect.Collector.runSparkJobs(Collector.scala:212)\n\tat org.apache.spark.sql.execution.collect.Collector.collect(Collector.scala:247)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:64)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:70)\n\tat org.apache.spark.sql.execution.ResultCacheManager.getOrComputeResult(ResultCacheManager.scala:497)\n\tat org.apache.spark.sql.execution.ResultCacheManager.getOrComputeResult(ResultCacheManager.scala:469)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollectResult(SparkPlan.scala:319)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3236)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3234)\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3334)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withCustomExecutionEnv$1.apply(SQLExecution.scala:89)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:175)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:84)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:126)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3333)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3234)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &quot;/databricks/spark/python/pyspark/worker.py&quot;, line 262, in main\n    process()\n  File &quot;/databricks/spark/python/pyspark/worker.py&quot;, line 257, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File &quot;/databricks/spark/python/pyspark/worker.py&quot;, line 183, in &lt;lambda&gt;\n    func = lambda _, it: map(mapper, it)\n  File &quot;&lt;string&gt;&quot;, line 1, in &lt;lambda&gt;\n  File &quot;/databricks/spark/python/pyspark/worker.py&quot;, line 79, in &lt;lambda&gt;\n    return lambda *a: f(*a)\n  File &quot;/databricks/spark/python/pyspark/util.py&quot;, line 55, in wrapper\n    return f(*args, **kwargs)\n  File &quot;&lt;command-2487822865328457&gt;&quot;, line 12, in deleteNull_castToFloat\nTypeError: object of type &apos;int&apos; has no len()\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:317)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:83)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:66)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:271)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:620)\n\tat org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$2$$anon$1.hasNext(InMemoryRelation.scala:157)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1092)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1083)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1018)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1083)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:809)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:347)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:298)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:42)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:336)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:300)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:42)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:336)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:300)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:42)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:336)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:300)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:42)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:336)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:300)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:112)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:384)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n</div>"]}}],"execution_count":17},{"cell_type":"code","source":["country_list = np.array(DF_fea.groupBy('process_country').count().orderBy('count').toPandas().process_country)"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["# process OS\nOS_0_10k = DF_select.groupBy(\"device_operatingSystem\").count().filter(\"count<10000\").select('device_operatingSystem')\nOS_0_10k = np.array(OS_0_10k.toPandas().device_operatingSystem)\n\n\ndef process_OS(x):\n  if x == None:\n    return 'Other'\n  elif x == '(not set)':\n    return 'Other'\n  elif x in OS_0_10k:\n    return 'Other_0_10k'\n  else:\n    return x\n  \nudf_process_OS = F.udf(process_OS, T.StringType())\nDF_fea = DF_fea.withColumn('process_OS', udf_process_OS('device_operatingSystem'))\ndisplay(DF_fea.groupBy(\"process_OS\").agg(F.count(\"process_OS\")).orderBy(\"count(process_OS)\"))"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["OS_list = np.array(DF_fea.groupBy('process_OS').count().orderBy('count').toPandas().process_OS)\nOS_list"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["# process geoNetwork_country\nimport numpy as np\nbrowser_5k_10k = DF_select.groupBy(\"device_browser\").count().filter(\"count>5000\").filter(\"count<10000\").select('device_browser')\nbrowser_0_5k = DF_select.groupBy(\"device_browser\").count().filter(\"count<=5000\").select('device_browser')\nbrowser_5k_10k = np.array(browser_5k_10k.toPandas().device_browser)\nbrowser_0_5k = np.array(browser_0_5k.toPandas().device_browser)\n\ndef process_browser(x):\n  if x == None:\n    return 'Other'\n  elif x == '(not set)':\n    return 'Other'\n  elif x in browser_5k_10k:\n    return 'Other_5k_10k'\n  elif x in browser_0_5k:\n    return 'Other_0_5k'\n  else:\n    return x\n  \nudf_process_browser = F.udf(process_browser, T.StringType())\nDF_fea = DF_fea.withColumn('process_browser', udf_process_browser('device_browser'))\ndisplay(DF_fea.groupBy(\"process_browser\").agg(F.count(\"process_browser\")).orderBy(\"count(process_browser)\"))"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["browser_list = np.array(DF_fea.groupBy('process_browser').count().orderBy('count').toPandas().process_browser)\nbrowser_list"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["# process channelGrouping\ndef process_CG(x):\n  if x == 'Affiliates':\n    return 'Affiliates'\n  if x == 'Paid Search':\n    return 'Paid Search'\n  if x == 'Display':\n    return 'Display'\n  if x == 'Referral':\n    return 'Referral'\n  if x == 'Direct':\n    return 'Direct'\n  if x == 'Social':\n    return 'Social'\n  if x == 'Organic Search':\n    return 'Organic Search'\n  else:\n    return 'Other'\n  \nudf_process_CG = F.udf(process_CG, T.StringType())\nDF_fea = DF_fea.withColumn('process_CG', udf_process_CG('channelGrouping'))\ndisplay(DF_fea.groupBy(\"process_CG\").agg(F.count(\"process_CG\")).orderBy(\"count(process_CG)\"))"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["CG_list = np.array(DF_fea.groupBy('process_CG').count().orderBy('count').toPandas().process_CG)\nCG_list"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["display(DF_fea)"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import OneHotEncoderEstimator\nall_col = ['revenue', 'process_CG', 'process_browser', 'process_country', 'process_OS', 'hour', 'weekday', 'month', 'year', 'hits', 'pageview', 'visitNumber']\ncategorical_features = ['process_CG', 'process_browser', 'process_country', 'process_OS', 'hour', 'weekday', 'month', 'year']\nconti_features = ['hits', 'pageview', 'visitNumber']\nstages = [] # stages in our Pipeline\n# One-hot encode cotegorical feature\nfor i in categorical_features:\n  stringIndexer = StringIndexer(inputCol=i, outputCol=i + \"_Index\").setHandleInvalid('skip')\n  #StringIndexer.handleInvalid('skip')\n  encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()],\n                                     outputCols=[i + \"_Vec\"])\n  stages += [stringIndexer, encoder]"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["assemblerInputs = [i + \"_Vec\" for c in categorical_features]+conti_features\nassembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\nstages += [assembler]"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["DF_train = DF_select.select('revenue', 'process_CG', 'process_browser', 'process_country', 'process_OS', 'hour', 'weekday', 'month', 'year', 'hits', 'pageview', 'visitNumber')\ndisplay(DF_train)"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["# Create a Pipeline.\npipeline = Pipeline(stages=stages)\npipelineModel = pipeline.fit(DF_train)\nmodel = pipelineModel.transform(DF_train)"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["from pyspark.ml.regression import GBTRegressor\nfrom pyspark.ml.evaluation import RegressionEvaluator\ngbt = GBTRegressor(featuresCol=\"features\", labelCol=\"revenue\", maxIter=20, maxDepth=30)\ntrain_data, test_data = model.randomSplit([.8,.2],seed=1234)"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\ncrossval.setEstimator(pipeline)\n\n# Let's tune over our dt.maxDepth parameter on the values 2 and 3, create a paramter grid using the ParamGridBuilder\nmaxDepth_grid = [10,20,30]\nmaxBins_grid = [30,50,70]\nmaxIter_grid = [15,20,25]\nparamGrid = (ParamGridBuilder()\n             .addGrid(gbt.maxDepth, maxDepth_grid)\n             .addGrid(gbt.maxBins, maxBins_grid)\n             .addGrid(gbt.maxIter, maxIter_grid)\n             .build())\n\n# Add the grid to the CrossValidator\ncrossval.setEstimatorParamMaps(paramGrid)\n\n# Now let's find and return the best model\ngbtModel = crossval.fit(train_data).bestModel"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["# Make prediction\npredictions = gbtModel.transform(test_data)\ndisplay(predictions.filter(\"revenue>0\").select(\"revenue\", \"prediction\"))"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["display(predictions.select(\"revenue\", \"prediction\"))"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["# Now let's compute an evaluation metric for our test dataset\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\n# Create an RMSE evaluator using the label and predicted columns\nregEval = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"revenue\", metricName=\"rmse\")\n\n# Run the evaluator on the DataFrame\nrmse = regEval.evaluate(predictions)\nrmse"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":36}],"metadata":{"name":"user","notebookId":2487822865328455},"nbformat":4,"nbformat_minor":0}
